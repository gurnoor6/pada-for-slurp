INFO - 04:42:17 - Started train_pada_experiment
/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Checkpoint directory /home/gurnoor/PADA/slurp_adaptation/runs/absa/recommendation/PADA/e10/b24/a0.2 exists and is not empty.
  warnings.warn(*args, **kwargs)
Global seed set to 212
GPU available: True, used: True
TPU available: None, using: 0 TPU cores

  | Name  | Type                       | Params
-----------------------------------------------------
0 | model | T5ForConditionalGeneration | 222 M 
-----------------------------------------------------
222 M     Trainable params
0         Non-trainable params
222 M     Total params
891.614   Total estimated model params size (MB)
Validation sanity check: 0it [00:00, ?it/s]Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]Validation sanity check:  50%|█████     | 1/2 [16:04<16:04, 964.06s/it]Validation sanity check: 100%|██████████| 2/2 [30:08<00:00, 928.17s/it]                                                                       Training: 0it [00:00, ?it/s]Training:   0%|          | 0/47 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/47 [00:00<?, ?it/s] Epoch 0:   0%|          | 0/47 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/gurnoor/PADA/slurp_adaptation/train.py", line 146, in <module>
    main()
  File "/home/gurnoor/PADA/slurp_adaptation/train.py", line 142, in main
    train_pada_experiment(args)
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/syct/timer_module.py", line 88, in wrapper_timer
    func_ret_val = func(*args, **kwargs)
  File "/home/gurnoor/PADA/slurp_adaptation/train.py", line 124, in train_pada_experiment
    trainer.fit(model)
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 513, in fit
    self.dispatch()
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 553, in dispatch
    self.accelerator.start_training(self)
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py", line 74, in start_training
    self.training_type_plugin.start_training(trainer)
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py", line 111, in start_training
    self._results = trainer.run_train()
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py", line 644, in run_train
    self.train_loop.run_training_epoch()
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py", line 484, in run_training_epoch
    for batch_idx, (batch, is_last_batch) in train_dataloader:
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/pytorch_lightning/profiler/profilers.py", line 86, in profile_iterable
    value = next(iterator)
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py", line 47, in _with_is_last
    last = next(it)
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/pytorch_lightning/trainer/supporters.py", line 470, in __next__
    return self.request_next_batch(self.loader_iters)
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/pytorch_lightning/trainer/supporters.py", line 484, in request_next_batch
    return apply_to_collection(loader_iters, Iterator, next)
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py", line 81, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 435, in __next__
    data = self._next_data()
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1085, in _next_data
    return self._process_data(data)
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1111, in _process_data
    data.reraise()
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/torch/_utils.py", line 428, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py", line 198, in _worker_loop
    data = fetcher.fetch(index)
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 73, in default_collate
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 73, in <dictcomp>
    return {key: default_collate([d[key] for d in batch]) for key in elem}
  File "/home/gurnoor/anaconda3/envs/pada/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py", line 81, in default_collate
    raise RuntimeError('each element in list of batch should be of equal size')
RuntimeError: each element in list of batch should be of equal size

